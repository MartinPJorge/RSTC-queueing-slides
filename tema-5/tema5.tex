\documentclass[xcolor={x11names}]{beamer}
\usetheme{Madrid}

\usepackage{amssymb}
\usepackage{ulem}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{multicol}
%\usepackage[x11names]{xcolor}
\usefonttheme{professionalfonts}


% Subfigures
\usepackage{caption}
\usepackage{subcaption}




%% MATH commands
\DeclareMathOperator{\Var}{Var}


%% THEOREMS
%\newtheorem{theorem}{Theorem}
\newtheorem{thm}{Teorema}[section] % the main one
% Definición
%\theoremstyle{definition}
\newtheorem{definicion}{Definición}[section]
\newtheorem{lema}{Lema}[section]



%% PFGplots %%
\usepackage{pgfplots}

%% Exponential distribution
\pgfmathdeclarefunction{exponential}{1}{%
  \pgfmathparse{(#1)*exp(-#1*x)}%
}
\pgfmathdeclarefunction{exponentialcdf}{1}{%
  \pgfmathparse{1-exp(-#1*x)}%
}

%% Poisson distribution
\pgfmathdeclarefunction{poiss}{1}{%
  \pgfmathparse{(#1^x)*exp(-#1)/(x!)}%
}

%% Normal distribution (#1=mu, #2=sigma)
% John D. Cook approx. https://tex.stackexchange.com/a/124629
\pgfmathdeclarefunction{normalcdf}{2}{%
  \pgfmathparse{1/(1 + exp(-0.07056*((x-#1)/#2)^3 - 1.5976*(x-#1)/#2))}%
}




\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}

%%%%%%%%%%
%% TIKZ %%
%%%%%%%%%%
\usepackage{tikz}
\usepackage{animate}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows, positioning, calc}
\usetikzlibrary{overlay-beamer-styles}
\usetikzlibrary{chains,shapes.multipart}
\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}       %                 ...customizing arrows
\usetikzlibrary{intersections}


%%%%%%%%%
%% PGF %%
%%%%%%%%%
\usepgfplotslibrary{fillbetween}


%%% Insert section name before the section %%%
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}



\title[Tema 5]{Tema 5: Introducción al {\sout{Teletráfico}}\\y a la Teoría de Colas}
\subtitle{Redes y Servicios de Telecomunicaciones (RSTC)\\
Grado en Ingeniería de Tecnologías y Servicios de Telecomunicación}
%\author{M. Saiful Bari\inst{1} \and Mr X\inst{2}}

\author{Jorge Martín Pérez\inst{1}}
\institute{
    \inst{1}
    Departamento de Ingeniería Telemática, Universidad Politécnica de Madrid
}

\date{\today}







%%%%%%%%%%%%%%%%%%%%
%%% SLIDES START %%%
%%%%%%%%%%%%%%%%%%%%
\begin{document}


%%% TITLE %%%
\frame{\titlepage}


\begin{frame}{Contenido}
    \tableofcontents
\end{frame}




\section{Introducción}
\begin{frame}{\secname}
    La teoría de colas modela:
    \begin{itemize}
        \item colas de supermercado;
        \item colas en gasolineras;
        \item colas en taquillas; o
        \item \textbf{colas de routers}.
    \end{itemize}

    \vfill
    Nos interesa saber:
    \begin{itemize}
        \item ¿cuánto vamos a esperar?; o
        \item la probabilidad de que esté llena la cola.
    \end{itemize}
\end{frame}



\begin{frame}{\secname}
    \begin{figure}
        \input{figs/mm1.tex}
    \end{figure}

    En una cola:
    \begin{itemize}
        \item llegan $\lambda$ [usuarios/sec]
        \item hay $q=4$ usuarios encolados;
        \item hay $n=5$ usuarios en total; y
        \item se sirven $\mu$ [usuarios/sec].
    \end{itemize}
\end{frame}


\begin{frame}{\secname}
    Problema:
    \begin{itemize}
        \item las llegadas; y
        \item tiempos de servicio
    \end{itemize}
    son \textbf{aleatorios}.

    \vfill

    \textit{Ejemplo}: la persona que nos
    atiende en caja tarda más o menos
    dependiendo de como de cansada esté,
    o de cuánto tarde la pasarela de pago
    (aleatorio).
\end{frame}






\section{Distribución Exponencial}

\begin{frame}{\secname}
    \begin{figure}
        \input{figs/exp-arrivals}
    \end{figure}
    El tiempo entre los usuarios que
    llegan a la cola \blue{$t$} se puede
    modelar con la \textbf{distribución exponencial}.
\end{frame}



\begin{frame}{\secname}
    \begin{definicion}[Distribución exponencial]
        Se dice que una variable aleatoria
        continua $t\in\mathbb{N}$ sigue una
        distribución exponencial si su
        función de densidad es:
        \begin{equation}
            f_t(\tau) = \mathbb{P}(t=\tau) = \lambda e^{-\lambda \tau}
        \end{equation}
        donde $\lambda>0$ es el parámetro
        que caracteriza la distribución.
    \end{definicion}
\end{frame}




\begin{frame}{\secname: propiedades}
    \begin{figure}
        \input{figs/exponential}
    \end{figure}


    \begin{itemize}
        \item \textbf{media}: $\mathbb{E}[t]=\tfrac{1}{\lambda}$
        \item \textbf{varianza}: $\Var[t]=\tfrac{1}{\lambda^2}$
    \end{itemize}
\end{frame}




\begin{frame}{\secname: ejemplo gasolinera}
    \textit{Ejemplo}: el tiempo medio
    que pasa un coche en un surtidor es
    $\mathbb{E}[\blue{t}]=\tfrac{1}{\lambda}=2$ [min].
    Por tanto $\lambda=\tfrac{1}{2}$ [coches/min].

    \vfill


    \begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \resizebox{\textwidth}{!}{%
             \input{figs/gasolinera-2min}%
         }
         \caption{$\mathbb{P}(\blue{t=2})=\tfrac{1}{2}\ e^{\frac{1}{2}\cdot \blue{2}}=0.18$}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \resizebox{\textwidth}{!}{%
             \input{figs/gasolinera-4min}%
         }
         \caption{$\mathbb{P}(\blue{t=4})=\tfrac{1}{2}\ e^{-\frac{1}{2}\cdot \blue{4}}=0.07$}
     \end{subfigure}
    \end{figure}

\end{frame}





\subsection{Propiedad sin memoria}


\begin{frame}{\secname: \subsecname}
Si ya han pasado {\color{DodgerBlue1}$s$ [sec]}, ¿cuál es
    la probabilidad de que tarde {\color{DodgerBlue4}$\tau$
    [sec]} más?:
    \begin{equation}
        \mathbb{P}(\red{t}>{\color{DodgerBlue1}s}+{\color{DodgerBlue4}\tau}|\ \red{t}>{\color{DodgerBlue1}s})
    \end{equation}

    \vfill

    \begin{figure}
        \input{figs/memoryless}
    \end{figure}
\end{frame}



\begin{frame}{\secname: \subsecname}
    Por la propiedad sin memoria de una
    exponencial tenemos que:
    \begin{equation}
        \mathbb{P}(\red{t}>{\color{DodgerBlue1}s}+{\color{DodgerBlue4}\tau}|\ \red{t}>{\color{DodgerBlue1}s}) = \mathbb{P}(\red{t}>{\color{DodgerBlue4}\tau})
    \end{equation}

    \vfill


    \begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \resizebox{\textwidth}{!}{%
             \input{figs/memoryless-no-animate}%
         }
         \caption{$\mathbb{P}(\red{t}>{\color{DodgerBlue1}s}+{\color{DodgerBlue4}\tau}|\ \red{t}>{\color{DodgerBlue1}s})$}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \resizebox{\textwidth}{!}{%
             \input{figs/memoryless-result}%
         }
         \caption{$\mathbb{P}(\red{t}>{\color{DodgerBlue4}\tau})$}
     \end{subfigure}
    \end{figure}
\end{frame}


\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: en media el surtidor
    de una gasolinera está ocupado 5 [min].
    Si el surtidor lleva
    {\color{DodgerBlue1}$s=1$ [min]} ocupado,
    ¿cuál es la probabildad de que esté ocupado
    {\color{DodgerBlue4}$\tau=3$ [min]} más?

    \vfill

    Por la propiedad sin memoria tenemos:
    \begin{equation*}
        \mathbb{P}(\red{t}>{\color{DodgerBlue1}s}+{\color{DodgerBlue4}\tau}|\ \red{t}>{\color{DodgerBlue1}s}) = \mathbb{P}(\red{t}>{\color{DodgerBlue4}\tau}) = \mathbb{P}(\red{t}>{\color{DodgerBlue4}3}) = \frac{1}{5}e^{-\frac{1}{5}\cdot3} = 0.11
    \end{equation*}

\end{frame}




\subsection{Mínimo de variables exponenciales}

\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: los compactos llegan
    a gasolinera con tasa
    {\color{Firebrick1}$\lambda_1=\tfrac{1}{4}$
    [coches/min]}, y los todoterreno con tasa
    {\color{Firebrick4}$\lambda_2=\tfrac{1}{8}$ [coches/min]}.

    \vfill

    ¿Con qué probabilidad llegua
    un coche cualquiera en 3 [min]?
\end{frame}



\begin{frame}{\secname: \subsecname}
    \begin{lema}[Mínimo de v.a. exponenciales]
        Sean las v.a.\footnote{v.a. significa variable aleatoria} exponenciales
        {\color{Firebrick1}$t_1$} y
        {\color{Firebrick4}$t_2$}, con
        tasas
        {\color{Firebrick1}$\lambda_1$} y
        {\color{Firebrick4}$\lambda_2$}; la v.a.
        $t=\min\{{\color{Firebrick1}t_1},{\color{Firebrick4}t_2}\}$
        se distribuye como una v.a. exponencial
        de tasa
        {$\lambda=\color{Firebrick1}\lambda_1+ \color{Firebrick4}\lambda_2$}.
    \end{lema}

    \vfill

    \textit{Demostración}:
    \begin{multline*}
        \mathbb{P}(t>\tau) =
        \mathbb{P}({\color{Firebrick1}t_1}>\tau)\mathbb{P}({\color{Firebrick4}t_2}>\tau)
        = \left(\int_\tau^\infty {\color{Firebrick1}\lambda_1}e^{-{\color{Firebrick1}\lambda_1} t}\ dt \right)
        \left(\int_\tau^\infty {\color{Firebrick4}\lambda_2}e^{-{\color{Firebrick4}\lambda_2} t}\ dt \right)\\
        = e^{-{\color{Firebrick1}\lambda_1} \tau} e^{-{\color{Firebrick4}\lambda_2} \tau} = e^{-({\color{Firebrick1}\lambda_1}+{\color{Firebrick4}\lambda_2})\tau}
        = e^{-\lambda \tau}
    \end{multline*}
\end{frame}




\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: los compactos llegan
    a gasolinera con tasa
    {\color{Firebrick1}$\lambda_1=\tfrac{1}{4}$
    [coches/min]}, y los todoterreno con tasa
    {\color{Firebrick4}$\lambda_2=\tfrac{1}{8}$ [coches/min]}.

    \vfill

    ¿Con qué probabilidad llega
    un coche cualquiera en 3 [min]?

    \begin{align*}
        1-\mathbb{P}(t>1)&=1-
        ({\color{Firebrick1}\lambda_1}+
        {\color{Firebrick4}\lambda_2})
        e^{-({\color{Firebrick1}\lambda_1}+
        {\color{Firebrick4}\lambda_2})\cdot 3}\\
        & =
        1 - \left({\color{Firebrick1}\frac{1}{4}}+
        {\color{Firebrick4}\frac{1}{8}}\right)
        e^{-({\color{Firebrick1}\frac{1}{4}}+
        {\color{Firebrick4}\frac{1}{8}})\cdot 3}
        = 0.12
    \end{align*}
\end{frame}







\subsection{Comparación de exponenciales}


\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: los compactos llegan
    a gasolinera con tasa
    {\color{Firebrick1}$\lambda_1=\tfrac{1}{4}$
    [coches/min]}, y los todoterreno con tasa
    {\color{Firebrick4}$\lambda_2=\tfrac{1}{8}$ [coches/min]}.

    \vfill

    ¿Cuál es la probabilidad de que llegue
    antes un compacto, es decir,
    (${\color{Firebrick1}t_1}<
    {\color{Firebrick4}t_2}$)?

    \begin{figure}
        \input{figs/arrival-before}
    \end{figure}

\end{frame}



\begin{frame}{\secname: \subsecname}
    \begin{lema}[Comparación de v.a. exponenciales]
        Sean las v.a. exponenciales
        {\color{Firebrick1}$t_1$} y
        {\color{Firebrick4}$t_2$}, con
        tasas
        {\color{Firebrick1}$\lambda_1$} y
        {\color{Firebrick4}$\lambda_2$}; se
        tiene que:
        \begin{equation}
            \mathbb{P}({\color{Firebrick1}t_1}<{\color{Firebrick4}t_2}) = \frac{{\color{Firebrick1}\lambda_1}}{{\color{Firebrick1}\lambda_1}+{\color{Firebrick4}\lambda_2}}
        \end{equation}
    \end{lema}

    \vfill

    \textit{Demostración}:
    \begin{equation*}
        \mathbb{P}({\color{Firebrick1}t_1}<{\color{Firebrick4}t_2})=
        \int_0^\infty \mathbb{P}({\color{Firebrick1}t_1}=t)\mathbb{P}({\color{Firebrick4}t_2}>t)\ dt = \int_0^\infty {\color{Firebrick1}\lambda_1} e^{-{\color{Firebrick1}\lambda_1} t} e^{-{\color{Firebrick4}\lambda_2} t}\ dt = \frac{{\color{Firebrick1}\lambda_1}}{{\color{Firebrick1}\lambda_1}+{\color{Firebrick4}\lambda_2}}
    \end{equation*}
\end{frame}





\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: los compactos llegan
    a gasolinera con tasa
    {\color{Firebrick1}$\lambda_1=\tfrac{1}{4}$
    [coches/min]}, y los todoterreno con tasa
    {\color{Firebrick4}$\lambda_2=\tfrac{1}{8}$ [coches/min]}.

    \vfill

    ¿Cuál es la probabilidad de que llegue
    antes un compacto, es decir,
    (${\color{Firebrick1}t_1}<
    {\color{Firebrick4}t_2}$)?

    \begin{figure}
        \input{figs/arrival-before}
    \end{figure}

    \begin{equation}
        \mathbb{P}({\color{Firebrick1}t_1}<{\color{Firebrick4}t_2}) = \frac{{\color{Firebrick1}\lambda_1}}{{\color{Firebrick1}\lambda_1}+{\color{Firebrick4}\lambda_2}} = 
        \frac{{\color{Firebrick1}\frac{1}{4}}}{{\color{Firebrick1}\frac{1}{4}}+{\color{Firebrick4}\frac{1}{8}}} = 0.67
    \end{equation}

\end{frame}







\section{Procesos de llegada de Poisson}


\subsection{Tiempos entre llegadas}

\begin{frame}{\secname: \subsecname}
    Buscamos una distribución que diga
    cómo de probable es que lleguen
    2 coches en 3 segundos:

    \vfill

    \begin{figure}
        \input{figs/arrivals}
    \end{figure}
\end{frame}



\begin{frame}{\secname: \subsecname}
    Si el tiempo entre llegadas es exponencial,
    sabemos la probabilidad de que lleguen
    ${\color{HotPink4}k=0}$
    coches en \red{$t=0.8$ [min]}.

    \vfill


    \begin{figure}
        \input{figs/arrivals-first-time}
    \end{figure}

    \vfill

    \begin{equation*}
        \mathbb{P}({\color{HotPink4}0\ \text{coches}}\ \text{en}\ \red{0.8 \text{ min}})=1-\mathbb{P}(\red{t>0.8})=1 - \lambda \red{0.8} e^{-\lambda \red{0.8}}
    \end{equation*}

\end{frame}



\subsection{Conteo}


\begin{frame}{\secname: \subsecname}
    Pero lo que queremos es contar
    el número de coches
    ${\color{HotPink4}N}(\blue{t})={\color{HotPink4}2}$
    que llegan en \blue{$t=3$ [min]},
    y saber qué probabilidad tiene
    $\mathbb{P}({\color{HotPink4}N}(\blue{t})={\color{HotPink4}2})$
    \begin{figure}
        \input{figs/arrivals-count}
    \end{figure}

\end{frame}






\begin{frame}{\secname: \subsecname}
    \begin{definicion}[Distribución de Poisson]
        Un proceso de llegadas
        ${\color{HotPink4}N}(\blue{t})$ con
        tasa \red{$\lambda$} es
        de Poisson si el tiempo entre
        llegadas se distribuye como una
        v.a. exponencial de media
        $\tfrac{1}{\red{\lambda}}$; y
        su función de densidad es

        \begin{equation}
            \mathbb{P}({\color{HotPink4}N}(\blue{t})={\color{HotPink4}k}) = \frac{(\red{\lambda}\blue{t})^{\color{HotPink4}k}\ e^{-\red{\lambda}\blue{t}}}{{\color{HotPink4}k}!}
        \end{equation}
    \end{definicion}

    \vfill



    \begin{figure}
        \centering
        \resizebox{.4\textwidth}{!}{%
            \input{figs/arrivals-count}%
        }
    \end{figure}

    \textit{Ejemplo}:
    $\mathbb{P}({\color{HotPink4}N}(\blue{3})={\color{HotPink4}2}) = \frac{(\red{\lambda}\blue{3})^{\color{HotPink4}2}\ e^{-\red{\lambda}\blue{3}}}{{\color{HotPink4}2}!}\underbrace{=}_{\red{\lambda=2/3}}0.27$

\end{frame}






%% \begin{frame}{\secname}
%%     Un proceso de llegadas de Poisson nos
%%     dice la probabilidad de que lleguen
%%     $k$ usuarios en $t$ segundos:
%%     \begin{equation}
%%         \mathbb{P}(\red{k}\ \text{usuarios en}\ \blue{t})=\frac{(\lambda \blue{t})^\red{k} e^{-\lambda \blue{t} }}{\red{k}!}
%%         \label{eq:poisson}
%%     \end{equation}
%%     donde $\lambda$ es la \textbf{tasa}
%%     de llegadas [usuarios/sec].
%% 
%%     \vfill
%% 
%%     \textit{Ejemplo}: si la tasa
%%     de llegada es $\lambda=5$ [usuarios/sec],
%%     la probabilidad de
%%     que lleguen $\red{k=3}$ usuarios en $\blue{t=2}$~sec es
%%     $\tfrac{(5\cdot2)^3 e^{-5\cdot2}}{3!}=0.0075$.
%% \end{frame}




\begin{frame}{\secname: \subsecname}
    Propiedades de la distribución de Poisson:
    \begin{itemize}
        \item \textbf{media}: $\mathbb{E}[N(t)]=\lambda t$ usuarios
        \item \textbf{varianza}: $\Var[N(t)]=\lambda t$ usuarios\textsuperscript{2}
    \end{itemize}

    \begin{figure}
        \input{figs/poisson}
    \end{figure}
\end{frame}




\subsection{Agregado}
\begin{frame}{\secname: \subsecname}
    ¿Cómo se distribuyen las llegadas
    de \textbf{\color{Firebrick1}A} y
    \textbf{\color{Firebrick4}B} juntos?

    \begin{figure}
        \input{figs/palm-khintchine.tex}
    \end{figure}

    Vemos que:
    \begin{itemize}
        \item {\color{Firebrick1}$\lambda_A=\tfrac{3}{4}$} [coches/min], ya que {\color{HotPink1}$N_A(\blue{t=4\text{min}})=3$} [coches]
        \item {\color{Firebrick4}$\lambda_B=\tfrac{2}{4}$} [coches/min], ya que {\color{HotPink4}$N_B(\blue{t=4\text{min}})=2$} [coches]
    \end{itemize}

\end{frame}









\begin{frame}{\secname: \subsecname}

    \begin{lema}[Agregado procesos de Poisson]
        Sean
        \textbf{\color{Firebrick1}A} y
        \textbf{\color{Firebrick4}B} dos
        procesos de Poisson independientes
        con tasas 
        $\color{Firebrick1}\lambda_1$ y
        $\color{Firebrick4}\lambda_2$;
        el agregado es un proceso de Poisson
        de tasa
        ${\color{Firebrick3}\lambda}={\color{Firebrick1}\lambda_1}+{\color{Firebrick1}\lambda_2}$.
    \end{lema}


    \begin{figure}
        \resizebox{.3\textwidth}{!}{%
            \input{figs/palm-khintchine.tex}%
        }
    \end{figure}

    \textit{Demostración} (caso general $n$ procesos):
    \begin{equation}
        \mathbb{P}({\color{HotPink4}N}(\blue{t})=0)=\prod_i^n \mathbb{P}({\color{HotPink2}N_i}(\blue{t})=0)=\prod_i^n e^{-{\color{Firebrick1}\lambda_i} \blue{t}} = e^{-\sum_i^n {\color{Firebrick1}\lambda_i} \blue{t}} = e^{-{\color{Firebrick3}\lambda} \blue{t}}
    \end{equation}

\end{frame}







\begin{frame}{\secname: \subsecname}
    \begin{thm}[Palm-Khintchine \cite{amable}]
        Sea $\{{\color{HotPink2}N_i}(\blue{t})\}_i^n$ un conjunto de $n$
        procesos de llegada independientes
        con sendas tasas
        {\color{Firebrick1}$\lambda_i$}.
        La superposición de procesos
        \begin{equation}
            {\color{HotPink4}N}(\blue{t})=\sum_i^n {\color{HotPink2}N_i}(\blue{t}), \blue{t}\geq0
        \end{equation}
        tiende a un \textbf{proceso de Poisson}
        de tasa ${\color{Firebrick3}\lambda}=\sum_i {\color{Firebrick1}\lambda_i}$
        cuando $n\to\infty$, siempre y cuando
        se cumpla:
        \begin{enumerate}
            \item carga finita ${\color{Firebrick3}\lambda}<\infty$; y
            \item ningún proceso domine al
                agregado ${\color{Firebrick1}\lambda_i}<<{\color{Firebrick3}\lambda}$
        \end{enumerate}

        \label{th:palm}
    \end{thm}
    
\end{frame}





\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo (tma. Palm-Khintchine)}:
    los tiempos de llegada de coches dependen
    del color, y son independientes del de
    otros colores. Además:
    \begin{itemize}
        \item tiempo entre coches rojos
            ${\color{Firebrick1}\sim U(0,10\text{ [min]})}$
        \item tiempo entre coches granates
            ${\color{Firebrick2}\sim N(\mu=20\text{ [min]}, \sigma=1 \text{ [min]})}$
        \item $\ldots$
        \item tiempo entre coches fucsia ${\color{Firebrick4}\sim Geo(p=0.2)}$
    \end{itemize}

    \vfill


    \begin{figure}
        \resizebox{.6\textwidth}{!}{%
          \input{figs/palm-khintchine-example.tex}%
        }
    \end{figure}

    \vfill


    El agregado será un proceso Poisson de
    tasa ${\color{Firebrick3}\lambda}=
    \sum_i {\color{Firebrick1}\lambda_i}=
    {\color{Firebrick1}\frac{1}{5}} + {\color{Firebrick2}\frac{1}{20}} + \ldots + p$
\end{frame}







\subsection{PASTA}
\begin{frame}{\secname: \subsecname}
    ``Poisson Arrivals See Time Averages'' (PASTA)\footnote{Ejemplo de \cite[Figura 3.17]{amable}}

    \vfill

    \begin{figure}
        \input{figs/step-wise}
    \end{figure}

    En media, los \blue{valores} vistos por
    {\color{Firebrick1}llegadas} de Poisson
    es la media temporal $\overline{X}(t)$.

\end{frame}



\begin{frame}{\secname: \subsecname}
    \begin{lema}[PASTA]
        Sea \blue{$X(t)$} un proceso aleatorio,
        y \red{$Y$} la v.a. definida como el
        valor que toman las llegadas de Poisson
        al muestrear \blue{X(t)}, se tiene que:
        \begin{equation}
            \blue{\overline{X}(t)} = \red{\overline{Y}}
        \end{equation}
    \end{lema}
\end{frame}




\begin{frame}{\secname: \subsecname - ejemplo}

    Media temporal: 
    \begin{equation*}
        \blue{\overline{X}(t)}=\frac{1}{6}\int_0^6 X(t)\ dt =\frac{1}{6} (2\cdot2 + 1\cdot2 + 3\cdot2) = \blue{\frac{8}{6}}
    \end{equation*}

    Llegadas de Poisson:
    \begin{equation*}
        \red{\overline{Y}} = 2\cdot\mathbb{P}([0,2]) + 1 \cdot\mathbb{P}([2,4]) + 3\cdot \mathbb{P}([4,6])=2\cdot\frac{2}{6} + 1 \cdot\frac{2}{6}+ 3\cdot\frac{2}{6}=\red{\frac{8}{6}}
    \end{equation*}

    \vfill

    \begin{figure}
        \resizebox{.4\textwidth}{!}{%
            \input{figs/step-wise}%
        }
    \end{figure}

    \vfill

    con
    \begin{equation*}%
        \scriptstyle
        \mathbb{P}([0,2])=\frac{\mathbb{P}(N(0,2)=1)\mathbb{P}(N(2,4)=0)\mathbb{P}(N(4,6)=0)}{\mathbb{P}(N([0,6])=1)}
                          =\frac{\frac{(2\lambda)^1e^{-2\lambda}}{1!} \frac{(2\lambda)^0e^{-2\lambda}}{0!} \frac{(2\lambda)^0e^{-2\lambda}}{0!} }{\frac{(6\lambda)^1e^{-6\lambda}}{1!}}=\frac{2}{6}%
    \end{equation*}%
\end{frame}






\section{Sistema M/M/1}

\begin{frame}{\secname}
    La notación de Kendall define cómo es 
    una cola
    \begin{center}
        A/S/c/K/N/D 
    \end{center}
    donde:
    \begin{itemize}
        \item \textbf{A} es la v.a. del tiempo
            entre llegadas;
        \item \textbf{S} es la v.a. del tiempo
            de servicio;
        \item \textbf{c} es el número de
            servidores que atienden la cola;
        \item \textbf{K} es en tamaño de la cola;
        \item \textbf{N} es la cantidad de
            llegadas; y
        \item \textbf{D} es la política de
            encolado.
    \end{itemize}
\end{frame}




\begin{frame}{\secname}
    Un M/M/1 es una cola\footnote{en notación
    Kendall M/M/1/$\infty$/$\infty$/FIFO}
    como la de la figura.

    \begin{figure}
        \input{figs/mm1.tex}
    \end{figure}

    donde:
    \begin{itemize}
        \item el tiempo entre llegadas es
            exponencial (M);
        \item el tiempo de servicio es
            exponencial (M);
        \item hay un servidor atendiento (1); y
        \item la cola es infinita ($\infty$).
    \end{itemize}

\end{frame}




\begin{frame}{\secname}
    Queremos responder a preguntas cómo:
    \begin{itemize}
        \item ¿cuál es la probabilidad
            de esperar en cola?
        \item ¿cuánto esperaré en la cola? o
        \item ¿cuánto tardaré en ser servido?
    \end{itemize}

    \vfill

    Para ello modelamos la cola con una
    cadena de Markov.
\end{frame}



\subsection{Cadena de Markov en tiempo continuo}

\begin{frame}{\secname: \subsecname}
    \begin{figure}
        \input{figs/cadena-markov.tex}
    \end{figure}

    \begin{figure}
        \input{figs/mm1-three.tex}
    \end{figure}


    \vfill

    \begin{itemize}
        \item estados = \#usuarios en
            cola y servidor
        \item transiciones =
            tasa de llegada $\lambda$ /
            tasa de servicio $\mu$
    \end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%
%% Primera definición
%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame}{\secname: \subsecname}
%%     \begin{definicion}[Cadena de Markov en tiempo continuo\cite{amable}]
%%         Un proceso estocástico $\{N(t),\ t>0\}$
%%         es una cadena de Markov en tiempo
%%         contínuo si se cumple la propiedad
%%         de Markov:
%%         \begin{align}
%%             & \mathbb{P}\left(N(t)=i_t|\ N(t-1)=i_{t-1}, N(t-2)=i_{t-2}, \ldots, N(0)=i_0\right)\nonumber\\
%%             & = \mathbb{P}\left(N(t)=i_t|\ N(t-1)=i_{t-1}\right)
%%         \end{align}
%%         con $t>t-1>\ldots>0$ y
%%         $\{i_t,i_{t-1},\ldots,i_0\}$ una secuencia
%%         de estados cualquiera.
%%     \end{definicion}
%% \end{frame}





% Proceso sin memoria ilustración
%% \begin{frame}{\secname: \subsecname}
%%     \begin{figure}
%%         \input{figs/markovian-process.tex}
%%     \end{figure}
%% \end{frame}


\begin{frame}{\secname: \subsecname}
    \begin{definicion}[Cadena de Markov en tiempo continuo\cite{amable}]
        Un proceso estocástico
        $\{N(t),\ t>0\}$ es una cadena de Markov
        si cumple:
        \begin{enumerate}
            \item el tiempo de estancia
                en el estado $i$ sigue
                una v.a. exponencial
                independiente de tasa $\nu_i$; y
            \item el proceso pasa del estado
                $i$ al $j$ con una
                probabilidad $\pi_{ij}$
                que cumple
                $\sum_j \pi_{ij} = 1,\ \forall i$
        \end{enumerate}
    \end{definicion}
\end{frame}




\begin{frame}{\secname: \subsecname}
    ¿Cumple la cadena de un M/M/1 la condición
    de Markov? \pause \textbf{Sí}.

    \vfill

    \begin{enumerate}
        \item el tiempo de estancia $T$ en un
            estado se distribuye como una
            exponencial de tasa
            $\nu=\lambda_l+\lambda_s$:
            \begin{equation*}
                \mathbb{P}(T>\tau)=\mathbb{P}(\min\{t_l,t_s\}>\tau)=e^{-(\lambda_l+\lambda_s)\tau}
            \end{equation*}
            con $t_l,t_s$ las v.a.
            exponenciales del tiempo de
            llegada y servicio.

        \item la suma de probabilidades
            de transición es uno
            \begin{multline*}
                \mathbb{P}(N(t+1)=k+1|\ N(t)=k)
                + \mathbb{P}(N(t+1)=k-1|\ N(t)=k)\\
                = \mathbb{P}(t_l<t_s)
                + \mathbb{P}(t_s<t_l)
                = \frac{\lambda_l}{\lambda_l+\lambda_s}
                + \frac{\lambda_s}{\lambda_l+\lambda_s} = 1
            \end{multline*}
    \end{enumerate}

\end{frame}






\begin{frame}{\secname: \subsecname}
    ¿Es realista asumir tiempos exponenciales?
    \begin{itemize}
        \item \textbf{tiempos de llegada}: sí por
            el teorema de
            Palm-Khintchine~\ref{th:palm}.\pause
        \item \textbf{tiempos de servicio}:
            no, pero da expresiones cerradas
            y es una cota pesimista para altas
            fiabilidades
    \end{itemize}

    \vfill

    
    \begin{figure}
        \input{figs/compare-exp-norm-unif}
    \end{figure}
\end{frame}








\subsection{Probabilidades de estado}




\begin{frame}{\secname: \subsecname}
    La cadena de Markov va pasando
    por estados\footnote{\url{https://setosa.io/blog/2014/07/26/markov-chains/index.html}}.

    En el instante $t$ estaremos la
    probabilidad de estar en cada estado es,
    e.g.:
    \begin{equation*}
        \pmb{\pi}(t) = (\pi_0(t), \pi_1(t), \pi_2(t), \pi_3(t), \ldots) = (0.02, 0.12, 0.3, 0.07, \ldots)
    \end{equation*}

    \begin{figure}
        \input{figs/cadena-markov-probs.tex}
    \end{figure}


\end{frame}





\begin{frame}{\secname: \subsecname}

    ¿Cómo varía la probabilidad de estar
    en el estado $i$ tras $\varepsilon$ [sec]?

    \begin{multline}
        \frac{d}{dt} {\color{HotPink4}\pi_i(t)}=-{\color{HotPink4}\pi_i(t)}{\color{Firebrick3}\nu_i}
        + \sum_{j\neq i} \pi_j(t) \cdot \nu_j\pi_{j i}\\
        =-{\color{HotPink4}\pi_i(t)}{\color{Firebrick3}(\lambda+\mu)} 
        + {\color{DodgerBlue1}\pi_{i-1}(t) \cdot (\lambda+\mu)
    \frac{\lambda}{\lambda+\mu}}
    + {\color{DodgerBlue4}\pi_{i+1}(t) \cdot \overbrace{(\lambda+\mu)}^{\nu_{i+1}}
        \overbrace{\frac{\mu}{\lambda+\mu}}^{\pi_{i,i+1}(t)} }\\
        =-{\color{HotPink4}\pi_i(t)} {\color{Firebrick3}(\lambda+\mu)} 
        + {\color{DodgerBlue1}\pi_{i-1}(t) \lambda }
        + {\color{DodgerBlue4}\pi_{i+1}(t) \mu}
        \label{eq:derivative}
    \end{multline}


    \begin{figure}
        \input{figs/derivada-transicion.tex}
    \end{figure}

\end{frame}





\begin{frame}{\secname: \subsecname}
    Si $t\to\infty$, la cadena alcanza
    \cite{amable}
    una distribución estacionaria donde las
    probabilidades no varían:
    
    \begin{equation*}
        \lim_{t\to\infty} \frac{d}{dt}\pi_i(t) = 0,\quad \forall i
        \label{eq:null-derivative}
    \end{equation*}


    \vfill

    Nos referimos a la \textbf{distribución
    estacionaria} como
    $\pmb{\pi}=\lim_{t\to\infty} \pmb{\pi}(t)$.

    \textit{Ejemplo}:
    \begin{equation*}
        \pmb{\pi} = (\pi_0, \pi_1, \pi_2, {\color{HotPink4}\pi_3}, \ldots) = (0.12, 0.04, 0.17, {\color{HotPink4}0.06}, \ldots)
    \end{equation*}

\end{frame}




\subsection{Ecuaciones de equilibrio}

\begin{frame}{\secname: \subsecname}
    Usando \eqref{eq:derivative}
    y \eqref{eq:null-derivative} podemos
    definir la \textbf{ecuación de equilibrio}
    para encontrar la distribución estacionaria
    $\pmb{\pi}$:

    \begin{equation}
        0 = \pmb{\pi}Q = 
        (\pi_0, \pi_1, \ldots)
        \begin{pmatrix}
            -\lambda & \lambda & 0 & 0 & \ldots \\
            \mu & -(\lambda+\mu) & \lambda & 0 & \ldots \\
            0  & \mu & -(\lambda+\mu) & \lambda &  \ldots \\
            \vdots  & \vdots & \vdots & \vdots &  \ddots \\
        \end{pmatrix}
        \label{eq:equilibrio}
    \end{equation}
    con $q_{ij}=\nu_i \pi_{i j}$ es la entrada
    $(i,j)$ de la matriz de transición $Q$.

\end{frame}






\begin{frame}{\secname: \subsecname}
    De \eqref{eq:equilibrio} sacamos
    el sistema de ecuaciones de equilibrio:
    \begin{equation}
        \pi_i = \pi_{i-1} \frac{\lambda}{\mu},
        \quad \forall i>0
        \label{eq:equilibrio}
    \end{equation}
    que equivale a:
    \begin{equation}
        \pi_i = \pi_0 \rho^i, \quad \forall i>0
        \label{eq:equilibrio-potencia}
    \end{equation}
    donde $\rho=\tfrac{\lambda}{\mu}$
    es la \textbf{carga del sistema}.
\end{frame}


\begin{frame}{\secname: \subsecname}
    De~\eqref{eq:equilibrio-potencia}
    sacamos la
    probabilidad de que el sistema
    M/M/1 esté vacío:
    \begin{align}
        \pi_0 &= 1 - \sum_{i=1}^\infty \pi_i\label{eq:prob-cero}\\
              &= 1 - \sum_{i=1}^\infty \pi_0\rho^i\nonumber\\
              &= 1 - \pi_0\frac{1-\rho}{1-\rho}
        \left(\rho+\rho^2+\rho^3+\ldots  \right)\nonumber\\
        &= 1 - \pi_0\frac{1}{1-\rho}
        \lim_{\iota\to\infty}\left( \rho - \rho^2 + \rho^2 - \rho^3 + \rho^3 - \ldots - \rho^\iota \right)\nonumber\\
        &= 1 - \pi_0 \frac{\rho}{1-\rho}\nonumber
    \end{align}
    siempre y cuando $\rho<1$.
\end{frame}





\begin{frame}{\secname: \subsecname}
    Despejando en \eqref{eq:prob-cero}
    y \eqref{eq:equilibrio-potencia}
    obtenemos que

    \begin{lema}[Probabilidades estado M/M/1]
        En un sistema M/M/1 la probabilidad
        de estar en el estado $i$ es:
        \begin{equation}
            \pi_i =
            \begin{cases}
                1-\rho, & i=0\\
                (1-\rho)\rho^i, & i>0
            \end{cases}
        \end{equation}
        donde $\rho=\tfrac{\lambda}{\mu}$
        es la carga del sistema.
    \end{lema}
\end{frame}




\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: en una gasolinera llegan
    $\lambda$ [coches/min] a un surtidor
    que sirve a tasa $\mu=1$ [coches/min].

    ¿Cómo varia la probabilidad
    de tener $i=3$ coches en función de
    $\lambda$?

    \vfill

    \begin{figure}
        \input{figs/prob-zero}
    \end{figure}
\end{frame}



\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo (cont.)}: ¿cuántos coches
    aguanta el surtidor para que el 90\%
    de las veces tenga menos de 5 coches?

    \begin{equation*}
        \lambda: \sum_{i=0}^4 \pi_i = \sum_{i=0}^4 (1-\rho)\rho^i = 1-\rho^5  \geq 0.9
        \implies\lambda\leq \sqrt[5]{0.1} \text{ [coches/min]}
    \end{equation*}

    \begin{figure}
        \input{figs/prob-less-than-five}
    \end{figure}
\end{frame}




\subsection{Métricas famosas}

\begin{frame}{\secname: \subsecname}
    \begin{lemma}[Número medio de usuarios
        en un M/M/1]
        El número medio de usuarios en
        un sistema M/M/1 es
        \begin{equation}
            \mathbb{E}[N(t)]=\frac{\rho}{1-\rho}
        \end{equation}
    \end{lemma}

    \vfill

    \textit{Demostración}:
    \begin{equation*}
        \mathbb{E}[N(t)]=
        \sum_{i=0}^\infty i \pi_i
        =(1-\rho)\rho \sum_{i=1}^\infty i\rho^{i-1}
        = (1-\rho)\rho \frac{d}{dt}\sum_{i=1}^\infty \rho^i = \ldots = \frac{\rho}{1-\rho}
    \end{equation*}
\end{frame}



\begin{frame}{\secname: \subsecname}
    \begin{lemma}[Número medio de usuarios
        encolados en un M/M/1]
        El número medio de usuarios encolados
        en un sistema M/M/1 es
        \begin{equation}
            \mathbb{E}[Q(t)]=\frac{\rho^2}{1-\rho}
        \end{equation}

    \end{lemma}

    \vfill
    \textit{Demostración}:
    \begin{multline*}
        \mathbb{E}[Q(t)]=
        \sum_{i=1}^\infty (i-1) \pi_i
        = \sum_{i=1}^\infty i\pi_i -
        \sum_{i=1}^\infty \pi_i\\
        = \mathbb{E}[N(t)] - (1-\pi_0)
        = \frac{\rho}{1-\rho} - \rho
        = \frac{\rho^2}{1-\rho}
    \end{multline*}
\end{frame}




\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo (cont.)}:
    si llegan $\lambda=1$ [coches/min],
    ¿cómo de rápido debe ser el surtidor
    para que, en media,
    haya menos de 3 coches esperando?

    \begin{equation}
        \mu: \mathbb{E}[Q(t)] = \frac{1}{\mu^2-\mu}\leq 2 \implies \mu\geq \frac{1+\sqrt{3}}{2}\text{ [coches/min]}
    \end{equation}


    \begin{figure}
        \input{figs/less-than-three-queue}
    \end{figure}
\end{frame}






\begin{frame}{\secname: \subsecname}
    ¿Y si queremos sacar el tiempo 
    de espera en cola on en ser servido?
    \pause

    \begin{thm}[Teorema de Little]
        En un sistema de colas,
        la relación entre tiempo medio
        de servicio $\mathbb{E}[T(t)]$
        y número medio de usuarios es
        \begin{equation}
            \mathbb{E}[N(t)] =
            \mathbb{E}[T(t)]\cdot \lambda
        \end{equation}
        Del mismo modo, la relación
        entre tiempo medio de espera
        en cola $\mathbb{E}[W(t)]$ y
        número medio de usuario en colas es
        \begin{equation}
            \mathbb{E}[Q(t)] =
            \mathbb{E}[W(t)]\cdot \lambda
        \end{equation}
    \end{thm}
\end{frame}




\begin{frame}{\secname: \subsecname}
    \textit{Ejemplo}: ¿cuál es la
    cantidad máxima de coches
    que agunta la gasolinera
    para que, en media, un coche tarde
    menos de 5 [min] en repostar?

    \begin{equation}
        \lambda: \mathbb{E}[T(t)] =
        \frac{1}{\mu-\lambda} \leq 5
        \implies \lambda \leq \frac{4}{5}
        \text{ [coches/min]}
    \end{equation}

    \begin{figure}
        \input{figs/less-than-five-min-avg.tex}
    \end{figure}

\end{frame}



\begin{frame}[allowframebreaks]
        \frametitle{Referencias}
        \bibliographystyle{amsalpha}
        \bibliography{refs.bib}
\end{frame}


\end{document}
